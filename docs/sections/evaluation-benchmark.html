<!-- ---------- GENERALIZATION BENCHMARK ---------- -->
<section id="evaluation-benchmark" class="section">
  <div class="container main-content-with-sidebar">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3 has-text-left">Out-of-Distribution vs. Out-of-Domain Generalization</h2>
        <div class="content has-text-justified">
          <p>
            We evaluate ViGaL with two kinds of generalization:
          </p>
          <ul>
            <li>
              <strong>Out-of-distribution games</strong> (left), where models trained on our visual games are tested on unseen Atari games.
              <ul>
                <li>The training and testing data are within the same domain, games.</li>
              </ul>
            </li>
            <li>
              <strong>Out-of-domain reasoning tasks</strong> (right), where the same models are assessed on multimodal reasoning datasets spanning mathematics, 3D understanding in CLEVR+, geometric problem solving, and multi-discipline on MMMU series.
              <ul>
                <li>The training and testing data are from completely different domains, transfering from gameplay to math and multi-discipline questions.</li>
              </ul>
            </li>
          </ul>
          

          <img src="./resources/dataset_card.png" alt="Dataset Card" class="img-responsive">
        </div>
      </div>
    </div>
  </div>
</section> 